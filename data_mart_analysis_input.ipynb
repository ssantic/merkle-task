{"cells":[{"cell_type":"markdown","source":["## DATA MART ASSIGNMENT"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67b776ec-3e85-415a-8278-109934fca978"}}},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05b036e5-f7e7-46c8-a92b-4833ee4b7c7b"}}},{"cell_type":"code","source":["from functools import reduce\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import DoubleType, IntegerType, DateType, StringType, TimestampType, DateType\nfrom pyspark.sql import *"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b6e0e05-40b5-4959-a2c5-3befb6c27edc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Configs and Constants"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acf123de-f2a2-407a-80bc-af6c633ff885"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",'Legacy')\n\n# File location and type\nLIST_OF_TABLES = [ \n    \"user\",\n    \"order\",\n    \"item\",\n    \"event\"\n]\n\nRAW_PREFIX = \"_raw\"\nSTAGE_PREFIX = \"_stage\"\nS3_SOURCE_PREFIX = \"s3://data-mart-source/\"\nS3_OUTPUT_PREFIX = \"s3://data-mart-source/databricks_delta_lake/\"\nFILE_TYPE = \"csv\"\nINFER_SCHEMA = \"true\"\nFIRST_ROW_IS_HEADER = \"true\"\nDELIMETER = \",\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc5c6ff0-ca73-466e-9db6-57c7849214fe"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Layer 1\n▪ Contains external tables for all prerequisite files <br/>\n▪ All attributes are of STRING type. No transformations are applied"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2b5631d-957c-4f9d-86ab-31a50c978c27"}}},{"cell_type":"code","source":["# Iterating all tables.\nfor table_name in LIST_OF_TABLES:    \n    file_location = f\"{S3_SOURCE_PREFIX}{table_name}.{FILE_TYPE}\"\n\n    # The applied options are for CSV files. For other file types, these will be ignored.\n    df = spark.read.format(FILE_TYPE) \\\n      .option(\"header\", FIRST_ROW_IS_HEADER) \\\n      .option(\"sep\", DELIMETER) \\\n      .option(\"quote\", \"\\\"\") \\\n      .option(\"escape\", \"\\\"\") \\\n      .load(file_location)\n    \n    # store the data in parquet format and creating external tables.\n    df.write.mode(\"OVERWRITE\").option(\"path\", f\"{S3_OUTPUT_PREFIX}{table_name}{RAW_PREFIX}\").saveAsTable(f\"{table_name}{RAW_PREFIX}\")\n    \n    # unpersist the df from the memory\n    df.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ea713e3-a751-4993-8b85-11c7e2110ec2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Layer 2 \n▪ Contains all datasets from the first layer <br/>\n▪ All attributes have common naming convention <br/>\n▪ All attributes have proper datatypes based on the attribute name and common logic <br/>\n▪ All struct collection attributes are flattened and transformed to proper data <br/>\n▪ Fact tables are properly partitioned based on meaningful attributes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ee6cfec-16bb-4d86-9bde-b47c10548183"}}},{"cell_type":"code","source":["# All column names in snake case. (Following same convention for all attributes)\ncolumn_names = {\n    \"user\":  [\n                \"created_at\",\n                \"deleted_at\",\n                \"email_address\",\n                \"first_name\",\n                \"id\",\n                \"last_name\",\n                \"merged_at\",\n                \"parent_user_id\"\n            ],    \n    \"order\": [\n                \"invoice_id\",\n                \"line_item_id\",\n                \"user_id\",\n                \"item_id\",\n                \"item_name\",\n                \"item_category\",\n                \"price\",\n                \"created_at\",\n                \"paid_at\"\n            ],\n    \"item\": [\n                \"adjective\",\n                \"category\",\n                \"created_at\",\n                \"id\",\n                \"modifier\",\n                \"name\",\n                \"price\"\n            ],\n    \"event\": [\n                \"event_id\",\n                \"event_time\",\n                \"user_id\",\n                \"payload\"\n            ]\n}\n\nall_dataframes = {}\n\nfor table_name in LIST_OF_TABLES:\n    df_table=spark.read.table(f\"{table_name}{RAW_PREFIX}\")\n    old_columns = df_table.columns\n    new_columns = column_names[table_name]\n    stage_df = reduce(lambda df_table, idx: df_table.withColumnRenamed(old_columns[idx], new_columns[idx]), range(len(old_columns)), df_table)\n    all_dataframes[table_name] = stage_df\n\n\n#  All attributes have proper datatypes based on the attribute name and common logic\nall_dataframes[\"user\"] = all_dataframes[\"user\"] \\\n    .withColumn('created_year',year(to_timestamp(col('created_at'), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn('created_at', to_timestamp(col('created_at'), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn('deleted_at', to_timestamp(col('deleted_at'), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn('id', col('id').cast(IntegerType())) \\\n    .withColumn('merged_at', to_timestamp(col('merged_at'), \"yyyy-MM-dd HH:mm:ss\"))\n\nall_dataframes[\"order\"]= all_dataframes[\"order\"] \\\n    .withColumn('created_year',year(to_timestamp(col('created_at'), \"MM/dd/yyyy HH:mm\"))) \\\n    .withColumn('created_at', to_timestamp(col('created_at'), \"MM/dd/yyyy HH:mm\")) \\\n    .withColumn('paid_at', to_timestamp(col('paid_at'), \"MM/dd/yyyy HH:mm\")) \\\n    .withColumn('price', col('price').cast(IntegerType())) \\\n    .withColumn('item_id', col('item_id').cast(IntegerType())) \\\n    .withColumn('invoice_id', col('invoice_id').cast(IntegerType())) \\\n    .withColumn('user_id', col('user_id').cast(IntegerType())) \\\n    .withColumn('line_item_id', col('line_item_id').cast(IntegerType()))\n\nall_dataframes[\"item\"] = all_dataframes[\"item\"] \\\n    .withColumn('created_year',year(to_timestamp(col('created_at'), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn('created_at', to_timestamp(col('created_at'), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn('id', col('id').cast(IntegerType())) \\\n    .withColumn('price', col('price').cast(DoubleType()))\n\nall_dataframes[\"event\"] = all_dataframes[\"event\"] \\\n    .withColumn('created_year',year(to_timestamp(col('event_time'), \"yyyy-MM-dd HH:mm:ss\"))) \\\n    .withColumn('event_time', to_timestamp(col('event_time'), \"yyyy-MM-dd HH:mm:ss\")) \\\n    .withColumn('user_id', col('user_id').cast(IntegerType()))\n\n\nall_dataframes[\"event\"] = all_dataframes[\"event\"].select(col('event_id'),\n                        col('created_year'),\n                        col('event_time'),\n                        col('user_id'),\n                        get_json_object(col('`payload`'), '$.event_name').alias('event_name'),\n                        get_json_object(col('`payload`'), '$.parameter_name').alias('parameter_name'),\n                        get_json_object(col('`payload`'), '$.platform').alias('platform'),\n                        get_json_object(col('`payload`'), '$.parameter_value').alias('parameter_value'))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dceb43d8-1f68-444e-8cfe-21025b306c3e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# partitioning raw tables based on year \nfor all_df in all_dataframes.keys():\n    all_dataframes[all_df].write.option(\"path\", f\"{S3_OUTPUT_PREFIX}{all_df}{STAGE_PREFIX}\").partitionBy(\"created_year\").mode(\"OVERWRITE\").saveAsTable(f\"{all_df}{STAGE_PREFIX}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e361a09-2707-4ad8-866f-672bea5af4ae"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["d\n##Top_Buyers\n\n##### data mart with top 20 customers who contributed on the total sales the most with additional attributes:\no Total sales contributed <br/>\no Rank based on the total sales <br/>\no Last order creation date <br/>\no The overall most viewed item of a customer <br/>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"847cf179-4387-4a59-8c42-692e8200374b"}}},{"cell_type":"code","source":["#reading event_staging table\n\nevent_raw=spark.read.table('event_stage') \\\n              .select(col('event_id'),\n                        col('event_time'),\n                        col('user_id'),\n                      col('parameter_name'),\n                      col('parameter_value'))\n                        \nevent_raw.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc4023b7-6613-43cb-8d97-eb9cd690e27d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#extracting all the items viewed most by any user\ntop_buyers_event=event_raw.select(col('user_id'),\n                                 col('parameter_value'),\n                                 col('parameter_name')) \\\n                          .filter(col('parameter_name')=='item_id') \\\n                          .groupBy(col('user_id'),col('parameter_name'),col('parameter_value')) \\\n                          .agg(count(col('parameter_value')).alias('total_number_of_times_item_viewed')) \\\n                          .withColumn('most_viewed_item',dense_rank().over(Window.partitionBy(col('user_id')).orderBy(col('total_number_of_times_item_viewed')))) \\\n                          .filter(col('most_viewed_item')==1) \\\n                          .select(col('user_id'),\n                                 col('parameter_value').alias('item_id'))\ntop_buyers_event.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eaacd53-97ca-49fa-8c60-68497da5abf0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Aggregating data and calculating:\n# 1) total orders placed by users.\n# 2) total sales contributed by users.\n# 3) last order date by that user.\n# 4) rank of user based on total sales made.\n\ntop_buyers_raw=spark.read.table('order_stage') \\\n                    .select(col('user_id'),\n                            col('invoice_id'),\n                            col('price'),\n                            col('created_at')) \\\n                    .groupBy(col('user_id')) \\\n                    .agg(count(col('invoice_id')).alias('total_orders_placed'),sum(col('price')).alias('total_sales_contributed'),max(to_date(col('created_at'),\"MM/dd/yyyy HH:mm\")).alias('last_order_created')) \\\n                    .withColumn('rank_of_user_based_on_total_sales',dense_rank().over(Window.orderBy(col('total_sales_contributed').desc()))) \\\n                    .filter(col('rank_of_user_based_on_total_sales')<=20)\ntop_buyers_raw.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d910c0ea-d9d2-43a2-875f-ba7f94d30fe3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#appending most viewed item for that particular user\ntop_buyers=top_buyers_raw.join(top_buyers_event,['user_id'],how='left') \\\n                         .groupby('user_id','total_orders_placed','total_sales_contributed','rank_of_user_based_on_total_sales','last_order_created') \\\n                         .agg (collect_list('item_id').alias('most_viewed_item_id'))\ntop_buyers.limit(10).display()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abdb755a-32aa-4289-be64-4c61a229290e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Top_Items\n\n##### “top_items” data mart with all sold items with additional attributes:\no For every year (based on the created_at attribute): <br/>\n-- Total number of an items sold in a particular year <br/>\n-- Rank of an item based on the total number of items sold in a particular year <br/>\n-- Total sales from an item in a particular year <br/>\n-- Rank of an item based on the total sales in a particular year <br/>\no Total number of items sold in all years <br/>\no Rank of an item based on the total number of sales <br/>\no Total sales of an item in all years <br/>\no Rank of an item based on the total sales"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7435b2a-d49a-453f-b8c7-b0d3ce3e5b4b"}}},{"cell_type":"markdown","source":["### For a particular year\n takes *order_year* as parameter declared as a widget variable in the notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52e6a4c4-4d08-44c9-bd3f-2b9d87d11b1b"}}},{"cell_type":"code","source":["# Total number of items sold in a partuclar year\nitems_raw=spark.read.table('item_stage')\ntotal_number_of_items_sold_in_year=items_raw.select(col('id').alias('item_id'),\n                                                   year(to_timestamp(col('created_at'),\"MM/dd/yyyy HH:mm\")).alias('order_year')) \\\n                                            .groupBy(col('order_year')) \\\n                                            .agg(count(col('item_id')).alias('total_number_of_items_sold'))\ntotal_number_of_items_sold_in_year.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33d2cee7-f4ba-474e-a728-bcb0e6d63091"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Total sales from an item in a particular year\ntotal_sales_of_items_sold_in_year=items_raw.select(col('price'),\n                                                   year(to_timestamp(col('created_at'),\"MM/dd/yyyy HH:mm\")).alias('order_year')) \\\n                                            .groupBy(col('order_year')) \\\n                                            .agg(sum(col('price')).alias('total_sales_of_items_sold'))\ntotal_sales_of_items_sold_in_year.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c16df16-8734-436e-9abc-16c4dcf86f2a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Rank of an item based on the total no of items sold in a particular year\n\nrank_items_total_no_sold=items_raw.select(col('id').alias('item_id'),\n                                                   year(to_timestamp(col('created_at'),\"MM/dd/yyyy HH:mm\")).alias('order_year')) \\\n                                            .groupBy(col('item_id'),col('order_year')) \\\n                                            .agg(count(col('item_id')).alias('total_number_of_items_sold')) \\\n                                            .withColumn('rank_of_an_item',dense_rank().over(Window.partitionBy('order_year').orderBy(col('total_number_of_items_sold').desc()))) \nrank_items_total_no_sold.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72816b76-5856-4e77-a2c8-8d9b74c571aa"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Rank of an item based on the total sales in a particular year\n\nrank_item_total_sales_made=items_raw.select(col('id').alias('item_id'),\n                                                   year(to_timestamp(col('created_at'),\"MM/dd/yyyy HH:mm\")).alias('order_year')) \\\n                                            .groupBy(col('item_id'),col('order_year')) \\\n                                            .agg(count(col('item_id')).alias('total_number_of_items_sold')) \\\n                                            .withColumn('rank_of_an_item',dense_rank().over(Window.partitionBy('order_year').orderBy(col('total_number_of_items_sold').desc()))) \nrank_item_total_sales_made.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfad26e2-3aee-480e-8efb-fad240bb768b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Total number of items sold in all years\n\ntotal_number_of_items_sold=items_raw.select(count(col('id').alias('item_id')).alias('total_number_of_items_sold_in_all_years'))\ntotal_number_of_items_sold.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c494610-2d4a-4db5-a3af-e493fdc358f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Total sales of an item in all years\ntotal_sales_in_all_years=items_raw.select(col('price'),col('id').alias('item_id')) \\\n                                  .groupBy(col('item_id')) \\\n                                  .agg(sum(col('price')).alias('total_sales_of_items_sold'))\ntotal_sales_in_all_years.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69a7317d-88db-4af5-b16a-19a04e167704"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Rank of an item based on the total number of sales\n\nitem_rank_no_item_sold=items_raw.select(col('id').alias('item_id')) \\\n                                            .groupBy(col('item_id')) \\\n                                            .agg(count(col('item_id')).alias('total_number_of_items_sold')) \\\n                                            .withColumn('rank_of_an_item',dense_rank().over(Window.orderBy(col('total_number_of_items_sold').desc())))\nitem_rank_no_item_sold.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10958313-d70f-448f-ab71-206e20cc6e40"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Rank of an item based on the total sales\n\nitem_rank_total_sales=items_raw.select(col('id').alias('item_id'),col('Price').alias('price')) \\\n                                            .groupBy(col('item_id')) \\\n                                            .agg(sum(col('price')).alias('total_sales_of_items_sold')) \\\n                                            .withColumn('rank_of_an_item',dense_rank().over(Window.orderBy(col('total_sales_of_items_sold').desc())))\nitem_rank_total_sales.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e20e67ea-d79a-4010-895f-bab41f77eb46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77913589-0d95-48e2-ba03-9121e1cdf39d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"All_Layers_Final","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3208341298004546}},"nbformat":4,"nbformat_minor":0}
